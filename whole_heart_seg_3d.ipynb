{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4YSFECmBVVMF"
      },
      "outputs": [],
      "source": [
        "import nibabel as nib\n",
        "from nibabel import processing\n",
        "\n",
        "from skimage import io\n",
        "from skimage.transform import resize\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import glob\n",
        "import warnings\n",
        "import shutil\n",
        "\n",
        "import SimpleITK as sitk\n",
        "# from nipype.interfaces.ants import N4BiasFieldCorrection\n",
        "from scipy import misc\n",
        "\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam, SGD\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import argparse\n",
        "import re\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zq2qaGKPVWoK",
        "outputId": "e8e01be9-48cc-43fc-beb1-f139b8d2fb85"
      },
      "outputs": [],
      "source": [
        "# check if GPU is available \n",
        "cuda = torch.cuda.is_available()\n",
        "print(\"GPU available:\", cuda)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6MFTwdPhVZup",
        "outputId": "7bc156bd-d4ab-4151-db18-de41c5fd7c13"
      },
      "outputs": [],
      "source": [
        "# grab all of the images and stack them. Do the same with the masks\n",
        "data_root_folder = Path('C:/Users/Susanna/Documents/Heart_Segmentation/data_3d/')\n",
        "scans_dir = Path('C:/Users/Susanna/Documents/Heart_Segmentation/lung/')\n",
        "masks_dir = Path('C:/Users/Susanna/Documents/Heart_Segmentation/masks/')\n",
        "\n",
        "# grab all the images in the directories\n",
        "scans_nifti = sorted(scans_dir.glob('*.nii'))\n",
        "masks_nifti = sorted(masks_dir.glob('*.nii'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-bSD8sWVb0a"
      },
      "outputs": [],
      "source": [
        "# get information from each nifti file\n",
        "\n",
        "def load_scan(scans_nifti):\n",
        "  result_data = []\n",
        "  result_img_format = []\n",
        "  paths = []\n",
        "  for scan in scans_nifti:\n",
        "    loaded_scan = nib.load(scan)\n",
        "    data = loaded_scan.get_fdata()\n",
        "    result_data.append(np.squeeze(data))\n",
        "    result_img_format.append(loaded_scan)\n",
        "    paths.append(scan)\n",
        "  return result_data, result_img_format, paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "58YIgRgJVdPU"
      },
      "outputs": [],
      "source": [
        "output_scans, scan_imgs, scan_list = load_scan(scans_nifti) # [load_scan(scan) for scan in scans_nifti]\n",
        "output_masks, mask_imgs, mask_list = load_scan(masks_nifti) # [load_scan(mask) for mask in masks_nifti]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bs3CoPH9ViLr"
      },
      "outputs": [],
      "source": [
        "scan_slices_train_dir = Path('C:/Users/Susanna/Documents/Heart_Segmentation/data_3d/train/scan/')\n",
        "mask_slices_train_dir = Path('C:/Users/Susanna/Documents/Heart_Segmentation/data_3d/train/mask/')\n",
        "\n",
        "scan_slices_valid_dir = Path('C:/Users/Susanna/Documents/Heart_Segmentation/data_3d/valid/scan/')\n",
        "mask_slices_valid_dir = Path('C:/Users/Susanna/Documents/Heart_Segmentation/data_3d/valid/mask/')\n",
        "\n",
        "scan_slices_test_dir = Path('C:/Users/Susanna/Documents/Heart_Segmentation/data_3d/test/scan/')\n",
        "mask_slices_test_dir = Path('C:/Users/Susanna/Documents/Heart_Segmentation/data_3d/test/mask/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vDaVhMrfesY1"
      },
      "outputs": [],
      "source": [
        "num_train_scan = scan_slices_train_dir.glob('*.nii.gz')\n",
        "num_train_scan = list(num_train_scan)\n",
        "\n",
        "num_valid_scan = scan_slices_valid_dir.glob('*.nii.gz')\n",
        "num_valid_scan = list(num_valid_scan)\n",
        "\n",
        "num_test_scan = scan_slices_test_dir.glob('*.nii.gz')\n",
        "num_test_scan = list(num_test_scan)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PAW1tA9U8YMK"
      },
      "outputs": [],
      "source": [
        "# separate out scan number from organization for train/valid/test\n",
        "def get_ind_3d(path):\n",
        "  scan_num = int(re.findall(r'\\d+', path)[0])\n",
        "  if 'valid' in path:\n",
        "    start = 56\n",
        "  elif 'test' in path:\n",
        "    start = 72\n",
        "  else:\n",
        "    start = 1\n",
        "  abs_ind = start + scan_num - 1\n",
        "  return abs_ind"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lLJbKr7883kP"
      },
      "outputs": [],
      "source": [
        "class BasicDataset(TensorDataset):\n",
        "\n",
        "    def __init__(self, folder, n_sample=None):\n",
        "        self.folder = os.path.join(data_root_folder, folder)\n",
        "        self.imgs_dir = os.path.join(self.folder, 'scan')\n",
        "        self.masks_dir = os.path.join(self.folder, 'mask')\n",
        "\n",
        "\n",
        "        self.imgs_file = sorted(glob.glob(os.path.join(self.imgs_dir, '*.nii.gz')))\n",
        "        self.masks_file = sorted(glob.glob(os.path.join(self.masks_dir, '*.nii.gz')))\n",
        "\n",
        "\n",
        "        assert len(self.imgs_file) == len(self.masks_file), 'There are some missing images or masks in {0}'.format(folder)\n",
        "\n",
        "        # If n_sample is not None (It has been set by the user)\n",
        "        if not n_sample or n_sample > len(self.imgs_file):\n",
        "            n_sample = len(self.imgs_file)\n",
        "\n",
        "        self.n_sample = n_sample\n",
        "        self.ids = list([i+1 for i in range(n_sample)])\n",
        "\n",
        "    # This function returns the lenght of the dataset (AKA number of samples in that set)\n",
        "    def __len__(self):\n",
        "        return self.n_sample\n",
        "\n",
        "    \n",
        "    def __getitem__(self, i):\n",
        "        idx = self.ids[i]\n",
        "        scan_path = os.path.join(self.imgs_dir, f\"scan_{str(i+1)}.nii.gz\")\n",
        "        mask_path = os.path.join(self.imgs_dir, f\"mask_{str(i+1)}.nii.gz\")\n",
        "\n",
        "\n",
        "        scan_num_abs  = get_ind_3d(scan_path)\n",
        "\n",
        "        img = nib.load(os.path.join(self.imgs_dir, f\"scan_{str(i+1)}.nii.gz\")).get_fdata()\n",
        "        mask = nib.load(os.path.join(self.masks_dir, f\"mask_{str(i+1)}.nii.gz\")).get_fdata()\n",
        "\n",
        "        img = img[:,:,0:32]\n",
        "        mask = mask[:,:,0:32]\n",
        "\n",
        "        scan_max = np.max(img)\n",
        "        img = np.array(img) / scan_max\n",
        "\n",
        "        # Add an axis to the mask array so that it is in [channel, width, height] format.\n",
        "        img = np.expand_dims(img, axis=0)\n",
        "        mask = np.expand_dims(mask, axis=0)\n",
        "\n",
        "\n",
        "\n",
        "        return {\n",
        "            'scan': torch.from_numpy(img).type(torch.FloatTensor),\n",
        "            'mask': torch.from_numpy(mask).type(torch.FloatTensor),\n",
        "            'img_id': idx,\n",
        "            'scan_num': scan_num_abs,\n",
        "            'max_pixel': scan_max\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 699
        },
        "id": "i80hNGuP9Ygs",
        "outputId": "09c33dca-e245-40ad-cce0-af4e29969e9d"
      },
      "outputs": [],
      "source": [
        "# Create train, validation, and test dataset instances\n",
        "train_dataset = BasicDataset('train')\n",
        "valid_dataset = BasicDataset('valid')\n",
        "test_dataset = BasicDataset('test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50sgzev7GPkg"
      },
      "outputs": [],
      "source": [
        "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=2, pin_memory=True)\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size=1, num_workers=2, pin_memory=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=1, num_workers=2, pin_memory=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o2GI5VKiGeKg"
      },
      "outputs": [],
      "source": [
        "class DoubleConv(nn.Module):\n",
        "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv3d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm3d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv3d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm3d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.double_conv(x);\n",
        "\n",
        "\n",
        "class Down(nn.Module):\n",
        "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.maxpool_conv = nn.Sequential(\n",
        "            nn.MaxPool3d(2),\n",
        "            DoubleConv(in_channels, out_channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.maxpool_conv(x)\n",
        "        return y;\n",
        "\n",
        "\n",
        "class Up(nn.Module):\n",
        "    \"\"\"Upscaling then double conv\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.up_conv = nn.Sequential(\n",
        "            nn.Upsample(scale_factor=2, mode='trilinear', align_corners=True),\n",
        "            nn.Conv3d(in_channels, out_channels, kernel_size=1, padding=0),\n",
        "        )\n",
        "        self.conv = DoubleConv(out_channels * 2, out_channels)\n",
        "\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.up_conv(x1)\n",
        "        x = torch.cat([x1, x2], dim=1)\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "class OutConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(OutConv, self).__init__()\n",
        "        self.conv_sigmoid = nn.Sequential(\n",
        "            nn.Conv3d(in_channels, out_channels, kernel_size=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv_sigmoid(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FArJHdLEGlgz"
      },
      "outputs": [],
      "source": [
        "# 3D U-Net implementation \n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, name, n_channels, n_classes):\n",
        "        super(UNet, self).__init__()\n",
        "        self.name = name\n",
        "        self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "\n",
        "        self.inputL = DoubleConv(n_channels, 64)\n",
        "        self.down1 = Down(64, 128)\n",
        "        self.down2 = Down(128, 256)\n",
        "        self.down3 = Down(256, 512)\n",
        "        self.down4 = Down(512, 1024)\n",
        "        self.up1 = Up(1024, 512)\n",
        "        self.up2 = Up(512, 256)\n",
        "        self.up3 = Up(256, 128)\n",
        "        self.up4 = Up(128, 64)\n",
        "        self.outputL = OutConv(64, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x0 = self.inputL(x)\n",
        "\n",
        "        x1 = self.down1(x0)\n",
        "        x2 = self.down2(x1)\n",
        "        x3 = self.down3(x2)\n",
        "        x4 = self.down4(x3)\n",
        "\n",
        "        x = self.up1(x4, x3)\n",
        "        x = self.up2(x, x2)\n",
        "        x = self.up3(x, x1)\n",
        "        x = self.up4(x, x0)\n",
        "\n",
        "        x = self.outputL(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-UVk1-FnGpKS",
        "outputId": "b3acd431-5cab-47d8-ad1e-89bbfa7e6911"
      },
      "outputs": [],
      "source": [
        "my_UNet = UNet('MyUNet', n_channels=1, n_classes=1)\n",
        "my_UNet.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "trMth156LFHr"
      },
      "outputs": [],
      "source": [
        "# use ADAM optimizer and BCE loss \n",
        "optimizer = torch.optim.Adam(my_UNet.parameters(), lr=0.001)\n",
        "loss_function = nn.BCELoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6U83X_MyLPsI",
        "outputId": "a2e460c3-12d3-44a7-a657-6b81ed599c14"
      },
      "outputs": [],
      "source": [
        "for batch in test_dataloader:\n",
        "    sample_batch = batch\n",
        "    break\n",
        "\n",
        "#  network prediction\n",
        "with torch.no_grad():\n",
        "    y_pred = my_UNet(sample_batch['scan'].cuda())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GnaPeyPjYn1M"
      },
      "outputs": [],
      "source": [
        "def dice_coeff_binary(y_pred, y_true):\n",
        "        \"\"\"Values must be only zero or one.\"\"\"\n",
        "        eps = 0.0001\n",
        "        inter = torch.dot(y_pred.view(-1), y_true.view(-1))\n",
        "        union = torch.sum(y_pred) + torch.sum(y_true)\n",
        "        return ((2 * inter.float() + eps) / (union.float() + eps)).cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6nHoGaHFYqqt"
      },
      "outputs": [],
      "source": [
        "# training loop\n",
        "def train_net(net, epochs, train_dataloader, valid_dataloader, optimizer, loss_function):\n",
        "\n",
        "    if not os.path.isdir('{0}'.format(net.name)):\n",
        "        os.mkdir('{0}'.format(net.name))\n",
        "\n",
        "    n_train = len(train_dataloader)\n",
        "    n_valid = len(valid_dataloader)\n",
        "\n",
        "    train_loss = list()\n",
        "    valid_loss = list()\n",
        "    train_dice = list()\n",
        "    valid_dice = list()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        net.train()\n",
        "        train_batch_loss = list()\n",
        "        train_batch_dice = list()\n",
        "\n",
        "        for i, batch in enumerate(train_dataloader):\n",
        "\n",
        "            # Load a batch \n",
        "            imgs = batch['scan'].cuda()\n",
        "            true_masks = batch['mask'].cuda()\n",
        "\n",
        "            # Produce the estimated mask \n",
        "            y_pred = net(imgs)\n",
        "\n",
        "            # Compute the loss for this batch \n",
        "            loss = loss_function(y_pred, true_masks)\n",
        "            batch_loss = loss.item()\n",
        "            train_batch_loss.append(batch_loss)\n",
        "\n",
        "            # Make the thresholded mask \n",
        "            pred_binary = (y_pred > 0.5).float()                    \n",
        "\n",
        "            # Compute the DICE score \n",
        "            batch_dice_score = dice_coeff_binary(pred_binary, true_masks)\n",
        "            train_batch_dice.append(batch_dice_score)\n",
        "\n",
        "\n",
        "            # Reset gradient \n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Compute losses\n",
        "            loss.backward()\n",
        "\n",
        "            # Update  weights\n",
        "            optimizer.step()\n",
        "\n",
        "            # Print the progress\n",
        "            print(f'EPOCH {epoch + 1}/{epochs} - Training Batch {i+1}/{n_train} - Loss: {batch_loss}, DICE score: {batch_dice_score}', end='\\r')\n",
        "\n",
        "        average_training_loss = np.array(train_batch_loss).mean()\n",
        "        average_training_dice = np.array(train_batch_dice).mean()\n",
        "        train_loss.append(average_training_loss)\n",
        "        train_dice.append(average_training_dice)\n",
        "\n",
        "\n",
        "\n",
        "        net.eval()\n",
        "        valid_batch_loss = list()\n",
        "        valid_batch_dice = list()\n",
        "\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for i, batch in enumerate(valid_dataloader):\n",
        "\n",
        "                # Load a batch and pass it to the GPU\n",
        "                imgs = batch['scan'].cuda()\n",
        "                true_masks = batch['mask'].cuda()\n",
        "\n",
        "                # Produce the estimated mask using current weights\n",
        "                y_pred = net(imgs)\n",
        "\n",
        "                # Compute the loss for this batch and append it to the epoch loss\n",
        "                loss = loss_function(y_pred, true_masks)\n",
        "                batch_loss = loss.item()\n",
        "                valid_batch_loss.append(batch_loss)\n",
        "\n",
        "                # Make the thresholded mask to compute the DICE score\n",
        "                pred_binary = (y_pred > 0.5).float()                    # You can change the probablity threshold!\n",
        "\n",
        "                # Compute the DICE score for this batch and append it to the epoch dice\n",
        "                batch_dice_score = dice_coeff_binary(pred_binary, true_masks)\n",
        "                valid_batch_dice.append(batch_dice_score)\n",
        "\n",
        "                # Print the progress\n",
        "                print(f'EPOCH {epoch + 1}/{epochs} - Validation Batch {i+1}/{n_valid} - Loss: {batch_loss}, DICE score: {batch_dice_score}', end='\\r')\n",
        "\n",
        "        average_validation_loss = np.array(valid_batch_loss).mean()\n",
        "        average_validation_dice = np.array(valid_batch_dice).mean()\n",
        "        valid_loss.append(average_validation_loss)\n",
        "        valid_dice.append(average_validation_dice)\n",
        "\n",
        "        print(f'EPOCH {epoch + 1}/{epochs} - Training Loss: {average_training_loss}, Training DICE score: {average_training_dice}, Validation Loss: {average_validation_loss}, Validation DICE score: {average_validation_dice}')\n",
        "\n",
        "    return train_loss, train_dice, valid_loss, valid_dice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IY1I5s3QYtR5",
        "outputId": "28c87692-0a97-4899-846e-66b093372681"
      },
      "outputs": [],
      "source": [
        "model_path = '/content/drive/MyDrive/Heart_Segmentation/3d_unet.pt'\n",
        "EPOCHS = 50\n",
        "train_loss, train_dice, valid_loss, valid_dice = train_net(my_UNet, EPOCHS, train_dataloader, valid_dataloader, optimizer, loss_function)\n",
        "torch.save(my_UNet.state_dict(), model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 806
        },
        "id": "DDewMNom0oYM",
        "outputId": "83d1be9c-9612-42a1-c8de-32c3cc090eef"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15,8))\n",
        "plt.suptitle('Learning Curve', fontsize=18)\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(np.arange(EPOCHS)+1, train_loss, '-o', label='Training Loss')\n",
        "plt.plot(np.arange(EPOCHS)+1, valid_loss, '-o', label='Validation Loss')\n",
        "# plt.xticks(np.arange(EPOCHS)+1)\n",
        "plt.xlabel('Epoch', fontsize=15)\n",
        "plt.ylabel('Loss', fontsize=15)\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(np.arange(EPOCHS)+1, train_dice, '-o', label='Training DICE score')\n",
        "plt.plot(np.arange(EPOCHS)+1, valid_dice, '-o', label='Validation DICE score')\n",
        "# plt.xticks(np.arange(EPOCHS)+1)\n",
        "plt.xlabel('Epoch', fontsize=15)\n",
        "plt.ylabel('DICE score', fontsize=15)\n",
        "plt.yticks(np.arange(0.6, 1, 0.05))\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oR6Pmh1t7t0X"
      },
      "outputs": [],
      "source": [
        "# store masks in array\n",
        "masks_test = []\n",
        "for batch in batches:\n",
        "    with torch.no_grad():\n",
        "        y_pred = my_UNet(batch['scan'].cuda())\n",
        "    for i in range(1):\n",
        "        pred_msk = ((y_pred.cpu().numpy()[i][0,:,:] > 0.5)).astype('uint8')\n",
        "        # print(pred_msk.shape)\n",
        "        # print(type(pred_msk))\n",
        "        masks_test.append(pred_msk)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "meMtVMZ05urw"
      },
      "outputs": [],
      "source": [
        "# storing outputs\n",
        "for i in range(len(masks_test)):\n",
        "    mask_nifti = mask_imgs[i + 71]\n",
        "    scan_nifti = scan_imgs[i + 71]\n",
        "    scan_array = output_scans[i+71]\n",
        "    scan_array_cropped = scan_array[:,:,0:32]\n",
        "    curr_mask = masks_test[i]\n",
        "\n",
        "    curr_scan_nifti = processing.conform(nib.Nifti1Image(scan_array_cropped, scan_nifti.affine), out_shape=scan_array_cropped.shape)\n",
        "    curr_mask_nifti = processing.conform(nib.Nifti1Image(curr_mask, mask_nifti.affine), out_shape=curr_mask.shape)# save curr mask as a nifti at output path\n",
        "\n",
        "    mask_nifti_path = '/Heart_Segmentation/output_3d/mask_'+  str(i + 1) + '.nii.gz'\n",
        "    scan_cropped_path = '/Heart_Segmentation/output_3d/scan_cropped_'+  str(i + 1) + '.nii.gz'\n",
        "    nib.save(curr, mask_nifti_path)\n",
        "    nib.save(curr_scan_nifti, scan_cropped_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4xHasB_7Q-V"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
